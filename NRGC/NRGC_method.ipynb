{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b614dd",
   "metadata": {},
   "source": [
    "# NRGC Method - Normalized Random Gene Clustering (NRGC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c13841",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8336e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import random\n",
    "from networkx.algorithms.community import girvan_newman\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb29d6",
   "metadata": {},
   "source": [
    "## Input: MAF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4d4748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 Hugo_Symbol  Entrez_Gene_Id         Center NCBI_Build  \\\n",
      "0           0     AADACL3               0  broad.mit.edu     GRCh37   \n",
      "1           1    ADAMTS19               0  broad.mit.edu     GRCh37   \n",
      "2           2       AGBL2               0  broad.mit.edu     GRCh37   \n",
      "3           3       ASCC3               0  broad.mit.edu     GRCh37   \n",
      "4           4         ATM               0  broad.mit.edu     GRCh37   \n",
      "\n",
      "  Chromosome  Start_Position  End_Position Strand       Consequence  ...  \\\n",
      "0          1        12785236      12785236      +  missense_variant  ...   \n",
      "1          5       129037220     129037220      +  missense_variant  ...   \n",
      "2         11        47726203      47726203      +  missense_variant  ...   \n",
      "3          6       101109870     101109870      +  missense_variant  ...   \n",
      "4         11       108218029     108218029      +  missense_variant  ...   \n",
      "\n",
      "  FILTER             ENSP ExAC_AF         CCDS   EXON ExAC_AF_OTH SAS_MAF  \\\n",
      "0      .  ENSP00000352268     NaN  CCDS41253.1    4/4         NaN     NaN   \n",
      "1      .  ENSP00000274487     NaN   CCDS4146.1  20/23         NaN     NaN   \n",
      "2      .  ENSP00000435582     NaN   CCDS7944.1   7/19         NaN     NaN   \n",
      "3      .  ENSP00000358159     NaN   CCDS5046.1  16/42         NaN     NaN   \n",
      "4      .  ENSP00000278616     NaN  CCDS31669.1  59/63         NaN     NaN   \n",
      "\n",
      "  Exon_Number MINIMISED PUBMED  \n",
      "0         4/4         1    NaN  \n",
      "1       20/23         1    NaN  \n",
      "2        7/19         1    NaN  \n",
      "3       16/42         1    NaN  \n",
      "4       59/63         1    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "maf_path = './inputs/prostate.maf'\n",
    "\n",
    "try:\n",
    "    maf_df = pd.read_csv(maf_path, sep='\\t', comment='#')\n",
    "    print(maf_df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo MAF: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55c4670",
   "metadata": {},
   "source": [
    "## Mutation Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894e58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_weights = {\n",
    "    'Nonsense_Mutation': 1.0,\n",
    "    'Missense_Mutation': 0.4,\n",
    "    'Splice_Site': 0.4,\n",
    "    'Frame_Shift_Del': 1.0,\n",
    "    'Frame_Shift_Ins': 1.0,\n",
    "    'In_Frame_Del': 0.4,\n",
    "    'In_Frame_Ins': 0.4,\n",
    "    '3\\'UTR': 0.2,\n",
    "    '5\\'UTR': 0.4,\n",
    "    'Nonstop_Mutation': 0.4,\n",
    "    'Translation_Start_Site': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06daae9",
   "metadata": {},
   "source": [
    "## Output: Weighted Mutation Matrix (WMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46ae22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: ./output/prostate.wmm\n"
     ]
    }
   ],
   "source": [
    "maf_df['Weight'] = maf_df['Variant_Classification'].map(mutation_weights)\n",
    "\n",
    "wmm = maf_df.groupby(['Tumor_Sample_Barcode', 'Hugo_Symbol'])['Weight'].mean().unstack(fill_value=0)\n",
    "\n",
    "output_dir = './output/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(output_dir, 'prostate.wmm')\n",
    "\n",
    "wmm.to_csv(output_file, sep='\\t')\n",
    "\n",
    "print(f\"Arquivo salvo em: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d036bed",
   "metadata": {},
   "source": [
    "## Output: Weighted Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdaa863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: ./output/normalized_gene_scores.nwf\n"
     ]
    }
   ],
   "source": [
    "def calculate_normalized_gene_scores(maf_df, wmm):\n",
    "\n",
    "    numero_pacientes = maf_df['Tumor_Sample_Barcode'].nunique()\n",
    "    pontuacao_bruta_genes = wmm.sum(axis=0) / numero_pacientes\n",
    "    pontuacao_normalizada_genes = pontuacao_bruta_genes / pontuacao_bruta_genes.max()\n",
    "\n",
    "    pontuacoes_df = pd.DataFrame({\n",
    "        'wf': pontuacao_bruta_genes,\n",
    "        'nwf': pontuacao_normalizada_genes\n",
    "    }).reset_index()\n",
    "\n",
    "    if 'index' in pontuacoes_df.columns:\n",
    "        pontuacoes_df.rename(columns={'index': 'Gene'}, inplace=True)\n",
    "    elif 'Hugo_Symbol' in pontuacoes_df.columns:\n",
    "        pontuacoes_df.rename(columns={'Hugo_Symbol': 'Gene'}, inplace=True)\n",
    "    else:\n",
    "        print(\"Coluna não encontrada.\")\n",
    "\n",
    "    pontuacoes_df.sort_values(by='Gene', inplace=True)\n",
    "    pontuacoes_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return pontuacoes_df, pontuacoes_df['nwf']\n",
    "\n",
    "pontuacoes_df, nwf = calculate_normalized_gene_scores(maf_df, wmm)\n",
    "\n",
    "output_file = os.path.join(output_dir, 'normalized_gene_scores.nwf')\n",
    "\n",
    "pontuacoes_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Arquivo salvo em: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75e6ae",
   "metadata": {},
   "source": [
    "## Output: Consensus Network UGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c97de698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo salvo em: ./output/ugn_gene_network.ugn\n"
     ]
    }
   ],
   "source": [
    "def calculate_frequency_pairs(file_paths):\n",
    "    exact_pair_frequency = defaultdict(int)\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                genes = line.strip().split()\n",
    "\n",
    "                for i in range(len(genes) - 1):\n",
    "                    pair = f\"{genes[i]} {genes[i+1]}\"\n",
    "                    exact_pair_frequency[pair] += 1\n",
    "\n",
    "    unordered_pair_frequency = defaultdict(int)\n",
    "    for pair, freq in exact_pair_frequency.items():\n",
    "        genes = pair.split()\n",
    "        unordered_pair = tuple(sorted(genes))\n",
    "        unordered_pair_frequency[unordered_pair] += freq\n",
    "\n",
    "    total_files = len(file_paths)\n",
    "    for pair in unordered_pair_frequency:\n",
    "        unordered_pair_frequency[pair] = unordered_pair_frequency[pair] / total_files\n",
    "\n",
    "    return unordered_pair_frequency\n",
    "\n",
    "def create_graph(unordered_pair_frequency):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for pair, freq in unordered_pair_frequency.items():\n",
    "        gene1, gene2 = pair\n",
    "        G.add_edge(gene1, gene2, weight=freq)\n",
    "\n",
    "    return G\n",
    "\n",
    "def save_graph_to_ugn(ugn, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for edge in ugn.edges(data=True):\n",
    "            gene1, gene2, data = edge\n",
    "            weight = data['weight']\n",
    "            file.write(f\"{gene1}\\t{gene2}\\t{weight}\\n\")\n",
    "    print(f\"Grafo salvo em: {output_file}\")\n",
    "\n",
    "file_paths = [\n",
    "    './inputs/ReactomeFI_preprocessed_122718.txt'\n",
    "]\n",
    "\n",
    "unordered_pair_frequency = calculate_frequency_pairs(file_paths)\n",
    "\n",
    "ugn = create_graph(unordered_pair_frequency)\n",
    "\n",
    "output_file = './output/ugn_gene_network.ugn'\n",
    "save_graph_to_ugn(ugn, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5b9c6",
   "metadata": {},
   "source": [
    "## Output: Gene Strength Spreading Network (GSSN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eaf72c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSSN salvo em: ./output/gssn_gene_network.gssn\n"
     ]
    }
   ],
   "source": [
    "def build_gssn(gene_network):\n",
    "    gssn = nx.DiGraph()\n",
    "    gene_network.remove_edges_from(nx.selfloop_edges(gene_network))\n",
    "    \n",
    "    max_weight = 0\n",
    "    for g_i, g_j in gene_network.edges:\n",
    "        weight_ij = gene_network[g_i][g_j][\"weight\"]\n",
    "        r_i = sum(gene_network[g_i][neighbor]['weight'] for neighbor in gene_network[g_i])\n",
    "        r_j = sum(gene_network[g_j][neighbor]['weight'] for neighbor in gene_network[g_j])\n",
    "        r_i_out = sum(gene_network[g_i][neighbor]['weight'] for neighbor in set(gene_network[g_i]) - set(gene_network[g_j]) - {g_j})\n",
    "        r_j_out = sum(gene_network[g_j][neighbor]['weight'] for neighbor in set(gene_network[g_j]) - set(gene_network[g_i]) - {g_i})\n",
    "\n",
    "        s_ij = (1 + (r_i * r_j_out)) * weight_ij\n",
    "        s_ji = (1 + (r_j * r_i_out)) * weight_ij\n",
    "\n",
    "        gssn.add_edge(g_i, g_j, weight=s_ij)\n",
    "        gssn.add_edge(g_j, g_i, weight=s_ji)\n",
    "\n",
    "        max_weight = max(max_weight, s_ij, s_ji)\n",
    "\n",
    "    for _, _, d in gssn.edges(data=True):\n",
    "        d['weight'] /= max_weight\n",
    "\n",
    "    return gssn\n",
    "\n",
    "def save_gssn_to_file(gssn, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for edge in gssn.edges(data=True):\n",
    "            gene1, gene2, data = edge\n",
    "            weight = data['weight']\n",
    "            file.write(f\"{gene1}\\t{gene2}\\t{weight:.6f}\\n\")\n",
    "    print(f\"GSSN salvo em: {output_file}\")\n",
    "\n",
    "gssn = build_gssn(ugn)\n",
    "\n",
    "output_file = './output/gssn_gene_network.gssn'\n",
    "save_gssn_to_file(gssn, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa577b4",
   "metadata": {},
   "source": [
    "## Result: NRGC Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6e23540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community Result salvo em: ./output/random_community_results_largest_component.output\n"
     ]
    }
   ],
   "source": [
    "def calculate_clustering_coefficient(graph, community):\n",
    "    subgraph = graph.subgraph(community)\n",
    "    return nx.average_clustering(subgraph, weight='weight')\n",
    "\n",
    "def normalized_weights(weights):\n",
    "    total_weight = sum(weights)\n",
    "    return [w / total_weight for w in weights]\n",
    "\n",
    "def expand_randomly_with_weight(graph, seed_node, visited):\n",
    "    cluster = {seed_node}\n",
    "    queue = [seed_node]\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.pop(0)\n",
    "        neighbors = list(graph.neighbors(current))\n",
    "        \n",
    "        if not neighbors:\n",
    "            continue\n",
    "\n",
    "        weights = [graph[current][neighbor]['weight'] for neighbor in neighbors]\n",
    "        probabilities = normalized_weights(weights)\n",
    "        \n",
    "        for neighbor, probability in zip(neighbors, probabilities):\n",
    "            if neighbor not in cluster and random.random() < probability:\n",
    "                cluster.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    visited.update(cluster)\n",
    "    return cluster\n",
    "\n",
    "def detect_communities_random(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    random.shuffle(nodes)\n",
    "    visited = set()\n",
    "    communities = []\n",
    "\n",
    "    for node in nodes:\n",
    "        if node not in visited:\n",
    "            community = expand_randomly_with_weight(graph, node, visited)\n",
    "            communities.append(community)\n",
    "\n",
    "    return communities\n",
    "\n",
    "def calculate_driver_precision(community, canonical_drivers):\n",
    "    drivers_in_community = [gene for gene in community if gene in canonical_drivers]\n",
    "    return len(drivers_in_community) / len(community) if community else 0\n",
    "\n",
    "def rank_communities_with_precision(graph, communities, canonical_drivers):\n",
    "    ranked_communities = []\n",
    "\n",
    "    for community in communities:\n",
    "        coefficient = calculate_clustering_coefficient(graph, community)\n",
    "        precision = calculate_driver_precision(community, canonical_drivers)\n",
    "\n",
    "        if coefficient > 0.20 and precision > 0.20:\n",
    "            ranked_communities.append((community, coefficient, precision))\n",
    "    \n",
    "    ranked_communities.sort(key=lambda x: (x[1], x[2]), reverse=True)\n",
    "    \n",
    "    return ranked_communities\n",
    "\n",
    "def save_communities_to_file(communities, output_file):\n",
    "    data = {\n",
    "        'Comunidade': [' '.join(sorted(comm)) for comm, _, _ in communities],\n",
    "        'Coeficiente': [f\"{coef:.2f}\" for _, coef, _ in communities],\n",
    "        'Precisão (%)': [f\"{prec:.2}\" for _, _, prec in communities]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file, sep='\\t', index=False, header=True)\n",
    "    print(f\"Community Result salvo em: {output_file}\")\n",
    "\n",
    "canonical_drivers_path = './inputs/canonical_drivers.txt'\n",
    "with open(canonical_drivers_path, 'r') as f:\n",
    "    canonical_drivers = set(line.strip() for line in f)\n",
    "\n",
    "largest_component = max(nx.strongly_connected_components(gssn), key=len)\n",
    "gssn_largest = gssn.subgraph(largest_component).copy()\n",
    "\n",
    "initial_communities = detect_communities_random(gssn_largest)\n",
    "ranked_communities = rank_communities_with_precision(gssn_largest, initial_communities, canonical_drivers)\n",
    "\n",
    "output_file = \"./output/random_community_results_largest_component.output\"\n",
    "save_communities_to_file(ranked_communities, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501611e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
